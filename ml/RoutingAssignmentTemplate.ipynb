{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7ba8c4-6b6d-4767-a3fe-3679f6b761b5",
   "metadata": {},
   "source": [
    "# Routing Assignment\n",
    "\n",
    "Congratulations! You just convinced a group of investors to put money into your startup competing against _Instacart_ for delivering specialty pet supplies for hamster grooming. Immediately upon publishing your website, the City of Dallas signs up to have a pallet of hamster grooming supplies delivered to \"1500 Marilla Street, Dallas, TX\" (Dallas City Hall--where all office animals are banned except hamsters). In addition, you received an order from Annamelissa Uhumehlemehe (a well-known childish hamster lady) who lives at \"11322 Cactus Lane, Dallas, TX\". You notice other addresses rolling in--_all in the City of Dallas_ (none in Garland, Grand Prarie, Ft. Worth, etc.). So you better hurry because you don't want to lose out on sales!\n",
    "\n",
    "You immediately go to the https://openstreetmap.org website and verify that these addresses are good addresses in OpenStreetMap including your warehouse at \"5351 Fults Blvd., Dallas, TX\" (just off Samuell Blvd., and South Buckner). One of your investors' requirements is that you not spend money on Google products or any other product. _You must use OpenStreetMap_ for your routing application.\n",
    "\n",
    "Your project is to write an application that optimizes deliveries to 20 addresses at a time in the City of Dallas using OpenStreetMap's API. You may pick 18 other addresses at random but, be careful to validate them in OpenStreetMap. Here are some address details:\n",
    "\n",
    "- Starting (warehouse) address is \"5351 Fults Blvd., Dallas, TX\"\n",
    "- You must include \"1500 Marilla Street, Dallas, TX\", and \"11322 Cactus Lane, Dallas, TX\" in your list of delivery addresses\n",
    "- You may pick 18 other addresses but they also have to process any verified list of OpenStreetMap-valid addresses\n",
    "\n",
    "Your application must:\n",
    "\n",
    "- Optimize the route starting from your warehouse address with the shortest distance to complete the route\n",
    "- Display the destinations and the distance between them (refer to the example in the assignment on Canvas)\n",
    "- Display a map with the most efficient route displayed (refer to the example in the assignment on Canvas)\n",
    "- Accept a list of up to 20 delivery addresses in Dallas and generate a new route list and map\n",
    "- Oh wait! The example on Canvas shows distances in km! We use **miles** here so don't forget to convert!\n",
    "\n",
    "The rubric for this assignment includes:\n",
    "\n",
    "- All required components are present and your program works as per the specifications (above)\n",
    "- Additional innovations may be (should be) added to differentiate your app from your classmates\n",
    "- Your program is well-commented and demonstrates that you know how it works\n",
    "- Any and all functions should include _docstrings_ that appear when the \"help( )\" function is called\n",
    "- The class presentation is excellent\n",
    "\n",
    "The cells below instantiate packages that may be of use as a starting place. You may alter them but remember: _you must use OpenStreetMap_ to complete this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53dce4dd-e38e-4769-9b66-3c3e0271b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c4e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLDeliveryRouter:\n",
    "    \"\"\"\n",
    "    Machine Learning based delivery route optimizer using reinforcement learning\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, discount_factor=0.95):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = self._build_model()\n",
    "        self.experience_buffer = []\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build neural network model for Q-learning\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(128, activation='relu', input_shape=(4,)),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(32, activation='relu'),\n",
    "            keras.layers.Dense(1)  # Q-value output\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def _get_state_features(self, current_location: Tuple[float, float], \n",
    "                           remaining_locations: List[Tuple[float, float]]) -> np.ndarray:\n",
    "        \"\"\"Extract relevant features for the current state\"\"\"\n",
    "        if not remaining_locations:\n",
    "            return np.zeros(4)\n",
    "            \n",
    "        # Calculate features\n",
    "        distances = [self._calculate_distance(current_location, loc) \n",
    "                    for loc in remaining_locations]\n",
    "        \n",
    "        features = [\n",
    "            np.mean(distances),  # Average distance to remaining locations\n",
    "            np.std(distances),   # Standard deviation of distances\n",
    "            min(distances),      # Distance to nearest location\n",
    "            len(remaining_locations)  # Number of remaining locations\n",
    "        ]\n",
    "        \n",
    "        return np.array(features).reshape(1, -1)\n",
    "    \n",
    "    def _calculate_distance(self, loc1: Tuple[float, float], \n",
    "                          loc2: Tuple[float, float]) -> float:\n",
    "        \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "        return np.sqrt((loc1[0] - loc2[0])**2 + (loc1[1] - loc2[1])**2)\n",
    "    \n",
    "    def _get_reward(self, distance: float, remaining_locations: int) -> float:\n",
    "        \"\"\"Calculate reward based on distance traveled and remaining locations\"\"\"\n",
    "        distance_penalty = -distance / 1000  # Convert to km\n",
    "        completion_bonus = 100 if remaining_locations == 0 else 0\n",
    "        return distance_penalty + completion_bonus\n",
    "    \n",
    "    def train(self, training_routes: List[List[Tuple[float, float]]], epochs=100):\n",
    "        \"\"\"Train the model on historical routes\"\"\"\n",
    "        print(\"Training ML model...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for route in training_routes:\n",
    "                current_loc = route[0]\n",
    "                remaining_locs = route[1:]\n",
    "                \n",
    "                while remaining_locs:\n",
    "                    # Get current state\n",
    "                    state = self._get_state_features(current_loc, remaining_locs)\n",
    "                    \n",
    "                    # Try each possible next location\n",
    "                    best_q = float('-inf')\n",
    "                    best_next = None\n",
    "                    \n",
    "                    for next_loc in remaining_locs:\n",
    "                        # Predict Q-value for this action\n",
    "                        next_state = self._get_state_features(next_loc, \n",
    "                                    [loc for loc in remaining_locs if loc != next_loc])\n",
    "                        q_value = self.model.predict(state, verbose=0)[0][0]\n",
    "                        \n",
    "                        if q_value > best_q:\n",
    "                            best_q = q_value\n",
    "                            best_next = next_loc\n",
    "                    \n",
    "                    # Move to best next location\n",
    "                    distance = self._calculate_distance(current_loc, best_next)\n",
    "                    reward = self._get_reward(distance, len(remaining_locs) - 1)\n",
    "                    \n",
    "                    # Store experience\n",
    "                    self.experience_buffer.append((state, reward, next_state))\n",
    "                    \n",
    "                    # Update current location and remaining locations\n",
    "                    current_loc = best_next\n",
    "                    remaining_locs.remove(best_next)\n",
    "                    \n",
    "                    # Train on mini-batch\n",
    "                    if len(self.experience_buffer) >= 32:\n",
    "                        self._train_on_batch()\n",
    "                        \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "    \n",
    "    def _train_on_batch(self, batch_size=32):\n",
    "        \"\"\"Train on a batch of experiences\"\"\"\n",
    "        if len(self.experience_buffer) < batch_size:\n",
    "            return\n",
    "            \n",
    "        # Sample random batch\n",
    "        batch = np.random.choice(len(self.experience_buffer), batch_size, replace=False)\n",
    "        states = []\n",
    "        targets = []\n",
    "        \n",
    "        for idx in batch:\n",
    "            state, reward, next_state = self.experience_buffer[idx]\n",
    "            \n",
    "            # Calculate target Q-value\n",
    "            if next_state.any():  # If not terminal state\n",
    "                next_q = self.model.predict(next_state, verbose=0)[0][0]\n",
    "                target = reward + self.discount_factor * next_q\n",
    "            else:\n",
    "                target = reward\n",
    "                \n",
    "            states.append(state[0])\n",
    "            targets.append(target)\n",
    "            \n",
    "        # Train model\n",
    "        self.model.train_on_batch(np.array(states), np.array(targets))\n",
    "    \n",
    "    def optimize_route(self, locations: List[Tuple[float, float]]) -> List[Tuple[float, float]]:\n",
    "        \"\"\"Find optimal route using trained model\"\"\"\n",
    "        if len(locations) <= 2:\n",
    "            return locations\n",
    "            \n",
    "        optimized_route = [locations[0]]  # Start with first location\n",
    "        remaining_locs = locations[1:]\n",
    "        current_loc = locations[0]\n",
    "        \n",
    "        while remaining_locs:\n",
    "            # Get current state\n",
    "            state = self._get_state_features(current_loc, remaining_locs)\n",
    "            \n",
    "            # Find best next location\n",
    "            best_q = float('-inf')\n",
    "            best_next = None\n",
    "            \n",
    "            for next_loc in remaining_locs:\n",
    "                next_state = self._get_state_features(next_loc, \n",
    "                            [loc for loc in remaining_locs if loc != next_loc])\n",
    "                q_value = self.model.predict(state, verbose=0)[0][0]\n",
    "                \n",
    "                if q_value > best_q:\n",
    "                    best_q = q_value\n",
    "                    best_next = next_loc\n",
    "            \n",
    "            # Add best location to route\n",
    "            optimized_route.append(best_next)\n",
    "            current_loc = best_next\n",
    "            remaining_locs.remove(best_next)\n",
    "        \n",
    "        return optimized_route\n",
    "    \n",
    "    def evaluate_route(self, route: List[Tuple[float, float]]) -> Dict:\n",
    "        \"\"\"Evaluate the quality of a route\"\"\"\n",
    "        total_distance = 0\n",
    "        for i in range(len(route) - 1):\n",
    "            total_distance += self._calculate_distance(route[i], route[i + 1])\n",
    "            \n",
    "        return {\n",
    "            'total_distance': total_distance,\n",
    "            'average_distance': total_distance / (len(route) - 1),\n",
    "            'num_stops': len(route)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18fdf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize router\n",
    "ml_router = MLDeliveryRouter()\n",
    "\n",
    "# Train on historical routes\n",
    "training_routes = [\n",
    "    [(lat1, lon1), (lat2, lon2), ...],  # Route 1\n",
    "    [(lat3, lon3), (lat4, lon4), ...],  # Route 2\n",
    "    # More routes...\n",
    "]\n",
    "ml_router.train(training_routes)\n",
    "\n",
    "# Optimize new route\n",
    "locations = [(lat1, lon1), (lat2, lon2), ...]  # Your locations\n",
    "optimized_route = ml_router.optimize_route(locations)\n",
    "\n",
    "# Evaluate route\n",
    "metrics = ml_router.evaluate_route(optimized_route)\n",
    "print(f\"Route metrics: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
